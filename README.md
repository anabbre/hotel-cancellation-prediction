# ğŸ¨ Hotel Cancellation Prediction

Este repositorio contiene un pipeline completo de Machine Learning para **predecir la cancelaciÃ³n de reservas de hotel**. El objetivo principal es dotar a los hoteles de una herramienta que les permita identificar proactivamente las reservas con alta probabilidad de ser canceladas, facilitando la toma de decisiones para optimizar la ocupaciÃ³n y los ingresos.

El proyecto abarca desde el anÃ¡lisis exploratorio de datos (EDA) hasta la comparaciÃ³n y evaluaciÃ³n de diversos modelos de aprendizaje automÃ¡tico, siguiendo las mejores prÃ¡cticas de modularidad y calidad de cÃ³digo.

---

**ğŸ“Š Â¡Accede al Reporte Interactivo de Resultados en GitHub Pages!**
[Haz clic aquÃ­ para ver el anÃ¡lisis detallado de mÃ©tricas, curvas ROC y conclusiones del modelo.](TU_URL_DE_GITHUB_PAGES_AQUI)

---
## ğŸ¯ Objetivo y Estrategia de EvaluaciÃ³n

El objetivo de este proyecto es construir un modelo predictivo robusto que ayude a los hoteles a minimizar las pÃ©rdidas asociadas a las cancelaciones de reservas. Predecir con antelaciÃ³n si una reserva serÃ¡ cancelada permite implementar estrategias como:

* **Overbooking inteligente:** Ajustar la capacidad de reservas para compensar las cancelaciones esperadas.
* **Ofertas personalizadas:** Contactar a clientes con alto riesgo de cancelaciÃ³n para ofrecerles incentivos y retener la reserva.
* **OptimizaciÃ³n de recursos:** Planificar mejor el personal y los recursos del hotel al tener una estimaciÃ³n mÃ¡s precisa de la ocupaciÃ³n real.

### JustificaciÃ³n de la MÃ©trica Principal: AUC-ROC

Dada la naturaleza del problema de negocio y el **desbalance de clases** inherente (generalmente hay menos cancelaciones que reservas completadas), hemos elegido la **Ãrea Bajo la Curva ROC (AUC-ROC)** como nuestra mÃ©trica de evaluaciÃ³n principal por las siguientes razones:

1.  **Manejo del Desbalance de Clases:** La AUC-ROC es robusta frente a datasets desbalanceados, ya que evalÃºa el rendimiento del clasificador en todos los posibles umbrales de clasificaciÃ³n. A diferencia de mÃ©tricas como la *Accuracy*, no se ve sesgada por la clase mayoritaria.
2.  **Capacidad de Ranking:** Mide la capacidad del modelo para distinguir entre las clases positiva (cancelaciÃ³n) y negativa (no cancelaciÃ³n). Una AUC-ROC alta indica que el modelo es bueno asignando una mayor probabilidad a las reservas que finalmente se cancelarÃ¡n, lo que es crucial para estrategias proactivas que requieren clasificar y priorizar riesgos.
3.  **Independencia del Umbral:** Permite evaluar el rendimiento general del modelo sin necesidad de fijar un umbral de clasificaciÃ³n especÃ­fico. Esto es vital en un contexto de negocio donde el coste de un Falso Positivo (FP) o Falso Negativo (FN) puede variar y el umbral Ã³ptimo podrÃ­a ajustarse dinÃ¡micamente. Al tener un modelo con alta AUC-ROC, la empresa puede elegir el umbral que mejor se adapte a su estrategia de negocio en un momento dado (por ejemplo, priorizando minimizar FN para evitar pÃ©rdidas, o minimizando FP para no contactar innecesariamente a clientes).

Si bien otras mÃ©tricas como Precision, Recall y F1-score se calculan y visualizan en el reporte (`02_reporting.ipynb`) para ofrecer una visiÃ³n completa, la AUC-ROC es el indicador fundamental de la capacidad discriminatoria del modelo para este caso de uso.

## ğŸ“ Estructura del Proyecto y Modularidad

El repositorio estÃ¡ organizado de forma modular para facilitar la claridad, el mantenimiento y la escalabilidad del proyecto. Cada componente tiene una responsabilidad especÃ­fica, permitiendo una fÃ¡cil comprensiÃ³n y modificaciÃ³n sin afectar otras partes del pipeline.
```
hotel-cancellation-prediction/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # Datos originales
â”‚   â””â”€â”€ processed/                 # Datos limpios y preprocesados
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_EDA.ipynb               # AnÃ¡lisis Exploratorio de Datos
â”‚   â””â”€â”€ 02_reporting.ipynb         # Reporte de resultados
â”œâ”€â”€ pictures/                      # ImÃ¡genes adicionales para el README
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ clean_data.py              # Limpieza inicial de datos
â”‚   â”œâ”€â”€ config.py                  # ParÃ¡metros y constantes globales
â”‚   â”œâ”€â”€ data_loader.py             # Funciones para cargar y dividir datos
â”‚   â”œâ”€â”€ preprocess.py              # Preprocesado de caracterÃ­sticas (imputaciÃ³n, escalado, encoding, SMOTE)
â”‚   â”œâ”€â”€ train.py                   # Entrenamiento de modelos inicial
â”‚   â”œâ”€â”€ tune.py                    # OptimizaciÃ³n de hiperparÃ¡metros (GridSearchCV)
â”‚   â”œâ”€â”€ evaluate.py                # EvaluaciÃ³n de mÃ©tricas de modelos
â”‚   â”œâ”€â”€ evaluate_final.py          # EvaluaciÃ³n final de modelos optimizados
â”‚   â”œâ”€â”€ visualize.py               # Funciones para guardar grÃ¡ficos y resÃºmenes
â”‚   â””â”€â”€ model_zoo/                 # Implementaciones modulares de los modelos (Model Zoo)
â”‚       â”œâ”€â”€ decision_tree.py
â”‚       â”œâ”€â”€ logistic_regression.py
â”‚       â”œâ”€â”€ gradient_boost.py
â”‚       â”œâ”€â”€ random_forest.py
â”‚       â””â”€â”€ mlp_tf.py              # Modelo MLP con TensorFlow/Keras
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ figures/                   # ImÃ¡genes generadas (Curvas ROC, Matrices de ConfusiÃ³n)
â”‚   â”œâ”€â”€ roc_csv/                   # CSVs con datos para Curvas ROC por modelo
â”‚   â”œâ”€â”€ auc_comparison.csv         # Resumen comparativo de AUC-ROC de todos los modelos
â”‚   â””â”€â”€ 02_reporting.html          # Reporte exportado del notebook 02_reporting
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ 02_reporting.html          # Contiene el reporte HTML para GitHub Pages
â”œâ”€â”€ models/                        # Modelos serializados (.joblib) y preprocesador
â”œâ”€â”€ README.md                      # Este archivo
â””â”€â”€ requirements.txt               # Dependencias del proyecto
```

### Concepto de "Model Zoo" y Modularidad

Para cumplir con el requisito de **modularidad y el uso de un "model zoo"**, cada algoritmo de Machine Learning se ha implementado en un archivo `.py` independiente dentro de la carpeta `src/model_zoo/`.

Cada archivo (`decision_tree.py`, `logistic_regression.py`, `gradient_boost.py`, `random_forest.py`, `mlp_tf.py`) contiene una funciÃ³n `build_model()` que devuelve una instancia del clasificador con parÃ¡metros por defecto o configurables vÃ­a `kwargs`. Esto permite:

* **ReutilizaciÃ³n:** Los modelos pueden ser importados y utilizados fÃ¡cilmente en diferentes partes del pipeline (entrenamiento, tuning, evaluaciÃ³n) sin duplicar cÃ³digo.
* **Facilidad de ExtensiÃ³n:** AÃ±adir un nuevo modelo es tan sencillo como crear un nuevo archivo en `src/model_zoo/` con su funciÃ³n `build_model()`.
* **Claridad:** La lÃ³gica de cada modelo estÃ¡ encapsulada, lo que mejora la legibilidad del cÃ³digo base.

### SerializaciÃ³n y Guardado de Modelos

Los modelos entrenados y el objeto `ColumnTransformer` utilizado para el preprocesado de datos se **serializan y guardan en el directorio `models/`**.

* El `ColumnTransformer` (encapsulando la imputaciÃ³n, escalado y One-Hot Encoding) se guarda durante la fase de preprocesado en `src/preprocess.py`. Esto asegura que las transformaciones aplicadas a los datos de entrenamiento sean replicables exactamente en los datos de validaciÃ³n y test, y crucialmente, en futuras inferencias con datos nuevos.
* Cada modelo entrenado (inicialmente en `src/train.py` y luego los modelos optimizados con `GridSearchCV` en `src/tune.py`) se guarda en formato `.joblib`. Esto permite cargar los modelos directamente para su evaluaciÃ³n o despliegue, sin necesidad de reentrenarlos.

Este enfoque de serializaciÃ³n garantiza la persistencia del estado del pipeline de preprocesado y de los modelos, facilitando la reproducibilidad y el despliegue en entornos de producciÃ³n.

## âš™ï¸ InstalaciÃ³n

Para configurar el entorno y ejecutar el proyecto, sigue estos pasos:

1.  **Clonar el repositorio:**
    ```bash
    git clone https://github.com/anabbre/hotel-cancellation-prediction.git
    cd hotel-cancellation-prediction
    ```

2.  **Crear y activar un entorno virtual:**
    ```bash
    python -m venv .venv
    # En Windows:
    .\.venv\Scripts\activate
    # En Linux/macOS:
    source .venv/bin/activate
    ```

3.  **Instalar las dependencias:**
    ```bash
    pip install -r requirements.txt
    ```

## ğŸš€ InstalaciÃ³n y EjecuciÃ³n del Pipeline

Una vez que el entorno estÃ© configurado, puedes ejecutar el pipeline completo para generar los datos limpios, entrenar y tunear los modelos, y producir los reportes de evaluaciÃ³n.

1.  **AsegÃºrate de que el dataset original** `dataset_practica_final.csv` estÃ© ubicado en `data/raw/`.

2.  **Ejecutar el pipeline completo:**
    El script `run_pipeline.py` orquesta la ejecuciÃ³n de los mÃ³dulos principales: `train.py`, `tune.py`, y `evaluate_final.py`.

    ```bash
    python run_pipeline.py
    ```
    Este comando realizarÃ¡ automÃ¡ticamente:
    * Carga y limpieza de datos (si no existe `dataset_limpio.csv` en `data/processed/`).
    * DivisiÃ³n del dataset en conjuntos de entrenamiento, validaciÃ³n y test.
    * Preprocesado de datos (imputaciÃ³n, escalado, One-Hot Encoding) y guardado del `ColumnTransformer`.
    * Entrenamiento inicial de todos los modelos del `Model Zoo` y guardado en `models/`.
    * OptimizaciÃ³n de hiperparÃ¡metros mediante `GridSearchCV` para cada modelo (excepto MLP) y guardado de los mejores modelos en `models/` (con sufijo `_best.joblib`).
    * EvaluaciÃ³n final de los modelos optimizados en el conjunto de test y guardado de las mÃ©tricas en `reports/final_metrics.csv` y `reports/auc_comparison.csv`.
    * GeneraciÃ³n de curvas ROC y matrices de confusiÃ³n en `reports/figures/`.

3.  **Explorar los resultados:**
    * Los notebooks `01_EDA.ipynb` y `02_reporting.ipynb` pueden ser abiertos con Jupyter Lab o VS Code para explorar el anÃ¡lisis y los resultados de forma interactiva.
    * El reporte HTML (`reports/02_reporting.html`) es una versiÃ³n estÃ¡tica del notebook de reporte.

## ğŸ“Š Resultados Clave y Valor de Negocio

Tras la evaluaciÃ³n de los diferentes modelos, el **Random Forest Classifier** ha demostrado ser el de mejor rendimiento para la predicciÃ³n de cancelaciones de hotel, obteniendo la siguiente mÃ©trica principal en el conjunto de test:

* **AUC-ROC: 0.953**

Para una visiÃ³n del rendimiento comparativo de los modelos en tÃ©rminos de AUC-ROC, consulta la grÃ¡fica a continuaciÃ³n. Random Forest muestra la curva mÃ¡s cercana a la esquina superior izquierda, indicando su superioridad:

![Comparativa de Curvas ROC de los Modelos](pictures/image.png)

### Impacto en el Negocio

El modelo de Random Forest, con una impresionante AUC-ROC de 0.954, representa una herramienta predictiva de gran valor estratÃ©gico para la gestiÃ³n hotelera. Su implementaciÃ³n se traduce en beneficios tangibles:

* **Mejor toma de decisiones:** Proporciona al equipo de gestiÃ³n de reservas una capacidad mejorada para identificar con alta precisiÃ³n quÃ© reservas tienen un mayor riesgo de ser canceladas. Esta inteligencia permite anticiparse a los eventos y no solo reaccionar a ellos.
* **MinimizaciÃ³n de pÃ©rdidas:** Al predecir cancelaciones, los hoteles pueden aplicar un overbooking inteligente y contactar proactivamente a clientes de alto riesgo con ofertas personalizadas o incentivos, reduciendo las vacantes inesperadas y maximizando la ocupaciÃ³n y los ingresos.
* **Eficiencia operativa:** La capacidad de prever las cancelaciones mejora significativamente la planificaciÃ³n de recursos (personal de limpieza, recepciÃ³n, gestiÃ³n de inventarios, preparaciÃ³n de habitaciones), lo que conduce a una asignaciÃ³n mÃ¡s eficiente y una operaciÃ³n mÃ¡s fluida.

En resumen, este sistema no solo predice la cancelaciÃ³n, sino que proporciona inteligencia accionable fundamental para una gestiÃ³n de reservas mÃ¡s eficiente, estratÃ©gica y, en Ãºltima instancia, mÃ¡s rentable.

Para un anÃ¡lisis detallado de todas las mÃ©tricas, matrices de confusiÃ³n y curvas ROC individuales, por favor, consulta el notebook `notebooks/02_reporting.ipynb` o el reporte HTML `reports/02_reporting.html`.

---

## âœï¸ Autores y Roles

- **Ana BelÃ©n Ballesteros** â€“ EDA, limpieza y preprocesado; modelado (src/model_zoo); reporting (notebooks, visualizaciones, README)
- **Victor MartÃ­nez** â€“ Tuning y evaluaciÃ³n (src/tune.py, src/evaluate.py); integraciones con TensorFlow; merges y pipeline refactor. 

Trabajo realizado de manera colaborativa en todas las fases: diseÃ±o del pipeline, desarrollo de scripts, validaciÃ³n de resultados y redacciÃ³n de la documentaciÃ³n.â€
